{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 10000\n",
      "Val dataset size is 1314\n",
      "Test dataset size is 7532\n"
     ]
    }
   ],
   "source": [
    "# Split train data into actual train and validation sets\n",
    "\n",
    "train_split = 10000\n",
    "train_data = newsgroup_train.data[:train_split]\n",
    "train_targets = newsgroup_train.target[:train_split]\n",
    "\n",
    "val_data = newsgroup_train.data[train_split:]\n",
    "val_targets = newsgroup_train.target[train_split:]\n",
    "\n",
    "test_data = newsgroup_test.data\n",
    "test_targets = newsgroup_test.target\n",
    "\n",
    "target_names = newsgroup_train.target_names\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: alt.atheism\n",
      "\n",
      "From: jbrown@batman.bmd.trw.com\n",
      "Subject: Re: Death Penalty / Gulf War\n",
      "Lines: 232\n",
      "\n",
      "In article <930419.115707.6f2.rusnews.w165w@mantis.co.uk>, mathew <mathew@mantis.co.uk> writes:\n",
      "> jbrown@batman.bmd.trw.com writes:\n",
      ">>In article <930414.121019.7E4.rusnews.w165w@mantis.co.uk>, mathew\n",
      ">><mathew@mantis.co.uk> writes:\n",
      ">>> Yes.  Fortunately we have right-thinking folks like your good self in power\n",
      ">>> and it was therefore deemed acceptable to slaughter tens or even hundreds o\n",
      ">>> thousands of Iraqis in order to liberate oil^H^H^HKuwait.  We won the war,\n",
      ">>> hurrah hurrah!\n",
      ">> \n",
      ">> The number of civilian Iraqi deaths were way over-exaggerated and \n",
      ">> exploited for anti-war emotionalism by the liberal news media.  The\n",
      ">> facts are that less Iraqis died in the Gulf War than did civilians \n",
      ">> in any other war of comparable size this century!\n",
      "> \n",
      "> Let's analyze this claim a little.  How is the \"size\" of a war defined?  By\n",
      "> number of participants?  Geographical area?  Number of countries involved? \n",
      "> Number of casualties?\n",
      "\n",
      "Size of armies, duration, numbers of casualties both absolute and as a\n",
      "percentage of those involved, geographical area and numbers of countries\n",
      "too, are all measures of size.  In this case I'd say the relevant\n",
      "statistic would be the number of combatants (total troops) compared to\n",
      "total casualties from among the total civilian population in the\n",
      "affected geographical area.\n",
      "\n",
      "> \n",
      "> Which other \"comparable\" wars are we talking about?\n",
      "\n",
      "Vietnam and Korea might make good comparisons.\n",
      "\n",
      "> \n",
      "> Which \"liberal news media\" are we talking about?\n",
      "> \n",
      "\n",
      "Western news in general, but in particular the American \"mass media\":\n",
      "CBS, NBC, ABC, etc.  The general tone of the news during the whole\n",
      "war was one of \"those poor, poor Iraqis\" along with \"look how precisely\n",
      "this cruise missile blew this building to bits\".\n",
      "\n",
      ">>                                                    This was due mostly\n",
      ">> to the short duration coupled with precise surgical bombing techniques\n",
      ">> which were technically possible only recently.\n",
      "> \n",
      "> I suspect that medical advances may have something to do with it too.\n",
      "\n",
      "I agree.\n",
      "\n",
      "> \n",
      ">> How about all the innocent people who died in blanket-bombing in WW2?\n",
      ">> I don't hear you bemoaning them!\n",
      "> \n",
      "> Perhaps because the topic hasn't cropped up.  If you want my opinion, I think\n",
      "> that the blanket bombing of German cities at the end of World War Two was the\n",
      "> most appalling act of wholesale slaughter this country has committed in\n",
      "> centuries.  Bomber Harris was no hero of mine.\n",
      "\n",
      "Perhaps so.  And maybe the atomic bomb was a mistake too.  But that's easy\n",
      "to say from our \"enlightened\" viewpoint here in the 90's, right?  Back\n",
      "then, it was *all-out* war, and Germany and Japan had to be squashed.\n",
      "After all, a million or more British had already died, hundreds of \n",
      "thousands of French, a couple hundread thousand or so Americans, and \n",
      "millions of Russians, not to mention a few million Jews, Poles, and \n",
      "other people of slavic descent in German concentration camps.  All \n",
      "things considered, the fire-bombings and the atomic bomb were\n",
      "essential (and therefore justified) in bringing the war to a quick\n",
      "end to avoid even greater allied losses.\n",
      "\n",
      "I, for one, don't regret it.\n",
      "\n",
      "> \n",
      ">>                                 War is never an exact science, but\n",
      ">> with smart bombs, it's becoming more exact with a smaller percentage\n",
      ">> of civilian casualties.  Sometimes mistakes are made; targets are\n",
      ">> misidentified; innocents die.  That's war the way it really is.\n",
      "> \n",
      "> Entrenched political rulers operating in their own selfish interests without\n",
      "> regard for the lives of other people, *that* is the way war really is.\n",
      "\n",
      "Sure.  And it's the people who suffer because of them.  All the more\n",
      "reason to depose these \"entrenched political rulers operating in their\n",
      "own selfish interests\"!  Or do you mean that this applies to the allies\n",
      "as well??\n",
      "\n",
      "> \n",
      "> Why all the fuss about Kuwait and not East Timor, Bosnia, or even Tibet?  If\n",
      "> Iraq is so bad, why were we still selling them stuff a couple of weeks before\n",
      "> we started bombing?\n",
      "\n",
      "I make no claim or effort to justify the misguided foreign policy of the\n",
      "West before the war.  It is evident that the West, especially America,\n",
      "misjudged Hussein drastically.  But once Hussein invaded Kuwait and \n",
      "threatened to militarily corner a significant portion of the world's\n",
      "oil supply, he had to be stopped.  Sure the war could have been\n",
      "prevented by judicious and concerted effort on the part of the West\n",
      "before Hussein invaded Kuwait, but it is still *Hussein* who is\n",
      "responsible for his decision to invade.  And once he did so, a\n",
      "strong response from the West was required.\n",
      "\n",
      "> \n",
      ">> Mathew, your sarcasm is noted but you are completely off-base here.\n",
      ">> You come off sounding like a complete peace-nik idiot, although I\n",
      ">> feel sure that was not your intent.\n",
      "> \n",
      "> What's your intent?  To sound like a Loving Christian?  Well, you aren't\n",
      "> doing a very good job of it.\n",
      "\n",
      "Well, it's not very \"loving\" to allow a Hussein or a Hitler to gobble up\n",
      "nearby countries and keep them.  Or to allow them to continue with mass\n",
      "slaughter of certain peoples under their dominion.  So, I'd have to\n",
      "say yes, stopping Hussein was the most \"loving\" thing to do for the\n",
      "most people involved once he set his mind on military conquest.\n",
      "> \n",
      ">> So the Iraqi war was wrong, eh?  I'm sure that appeasement would have\n",
      ">> worked better than war, just like it did in WW2, eh?\n",
      "> \n",
      "> Who even mentioned appeasement?  And what makes you think the situation is\n",
      "> even remotely analogous to World War Two?\n",
      "\n",
      "I mentioned it.\n",
      "\n",
      "If we hadn't intervened, allowing Hussein to keep Kuwait, then it would\n",
      "have been appeasement.  It is precisely the lessons the world learned\n",
      "in WW2 that motivated the Western alliance to war.  Letting Hitler take\n",
      "Austria and Czechoslavkia did not stop WW2 from happening, and letting\n",
      "Hussein keep Kuwait would not have stopped an eventual Gulf War to\n",
      "protect Saudi Arabia.\n",
      "\n",
      "> \n",
      ">>                                                           I guess we\n",
      ">> shouldn't have fought WW2 either -- just think of all those innocent\n",
      ">> German civilians killed in Dresden and Hamburg.\n",
      "> \n",
      "> Yes, do.  Germans are human too, you know.\n",
      "> \n",
      "\n",
      "Sure.  What was truly unfortunate was that they followed Hitler in\n",
      "his grandiose quest for a \"Thousand Year Reich\".  The consequences\n",
      "stemmed from that.\n",
      "\n",
      ">> Tyrants like Hussein *have* to be stopped.  His kind don't understand\n",
      ">> diplomacy; they only understand the point of a gun.  My only regret is\n",
      ">> that Bush wimped out and didn't have the military roll into Baghdad, so\n",
      ">> now Hussein is still in power and the Iraqi people's sacrifice (not to\n",
      ">> mention the 357 Americans who died) was for naught.\n",
      "> \n",
      "> I look forward to hearing your incisive comments about East Timor and Tibet.\n",
      "> \n",
      "What should I say about them?  Anything in particular?\n",
      "\n",
      "\n",
      ">> And as for poor, poor Rodney King!  Did you ever stop and think *why*\n",
      ">> the jury in the first trial brought back a verdict of \"not guilty\"?\n",
      "> \n",
      "> Yes.  Amongst the things I thought were \"Hmm, there's an awful lot of white\n",
      "> people in that jury.\"\n",
      "\n",
      "So?  It was the *policemen* on trial not Rodney King!!  And under American\n",
      "law they deserved a jury of *their* peers!  If there had been black\n",
      "officers involved, I'm sure their would have been black jurors too.\n",
      "This point (of allegedly racial motivations) is really shallow.\n",
      "\n",
      "> \n",
      ">> Those who have been foaming at the mouth for the blood of those\n",
      ">> policemen certainly have looked no further than the video tape.\n",
      ">> But the jury looked at *all* the evidence, evidence which you and I\n",
      ">> have not seen.\n",
      "> \n",
      "> When I see a bunch of policemen beating someone who's lying defenceless on\n",
      "> the ground, it's rather hard to imagine what this other evidence might have\n",
      "> been.\n",
      "\n",
      "So?  It's \"hard to imagine\"?  So when has Argument from Incredulity\n",
      "gained acceptance from the revered author of \"Constructing a Logical\n",
      "Argument\"?  Can we expect another revision soon??  :)  (Just kidding.)\n",
      "\n",
      "> \n",
      "> If there is some wonderful evidence, why is it seemingly being kept secret? \n",
      "> Why not tell everyone what it is?  Then everyone could say \"Oh, yes, you're\n",
      "> right, King deserved a good beating\", and we could all live happily ever\n",
      "> after.\n",
      "\n",
      "I have to admit that I wonder this too.  But *neither* the prosecution\n",
      "nor the defense is talking.  So one cannot conclude either way due to\n",
      "the silence of the principals.  \n",
      "\n",
      "> \n",
      ">> Law in this country is intended to protect the rights of the accused,\n",
      ">> whether they be criminals or cops.  One is not found guilty if there is\n",
      ">> a reasonable doubt of one's guilt, and only the jury is in a position\n",
      ">> to assess the evidence and render a verdict.\n",
      "> \n",
      "> Fine, but I'm still finding it hard to imagine what the \"reasonable doubt\"\n",
      "> was in this case.  I mean, the cops certainly seem to be beating someone\n",
      "> who's lying defenceless on the ground.  What's your explanation?  Mass\n",
      "> hallucination?  Orbital mind-control lasers?  Faked video footage?  Do tell.\n",
      "> \n",
      "\n",
      "OK.  It certainly seemed to me that there was excessive force involved.\n",
      "And frankly, the original \"not guilty\" verdict baffled me too.  But then\n",
      "I learned that the prosecution in the first case did not try to convict\n",
      "on a charge of excessive force or simple assault which they probably\n",
      "would have won, they tried to get a conviction on a charge of aggravated\n",
      "assault with intent to inflict serious bodily harm.  A charge, which\n",
      "news commentators said, was akin to attempted murder under California\n",
      "law.  Based on what the prosecution was asking for, it's evident that \n",
      "the first jury decided that the officers were \"not guilty\".  Note, \n",
      "not \"not guilty\" of doing wrong, but \"not guilty\" of aggravated assault \n",
      "with the *intent* of inflicting serious bodily harm.  The seeds of the \n",
      "prosecutions defeat were in their own overconfidence in obtaining a \n",
      "verdict such that they went for the most extreme charge they could.\n",
      "\n",
      "If the facts as the news commentators presented them are true, then\n",
      "I feel the \"not guilty\" verdict was a reasonable one.\n",
      "\n",
      "> \n",
      "> mathew\n",
      "> [ \"Thou shalt not kill... unless thou hast a pretty good reason for killing,\n",
      ">    in which case thou shalt kill, and also kill anyone who gets in the way,\n",
      ">    as unfortunately it cannot be helped.\"\n",
      ">                                    -- Jim Brown Bible for Loving Christians ]\n",
      "\n",
      "Thanks mathew, I like the quote.  Pretty funny actually.  (I'm a \n",
      "Monty Python fan, you know.  Kind of seems in that vein.)\n",
      "\n",
      "Of course, oversimplifying any moral argument can make it seem\n",
      "contradictory.  But then, you know that already.  \n",
      "\n",
      "Regards,\n",
      "\n",
      "Jim B.\n",
      "Loving Christian  :)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "import random\n",
    "ix = random.randint(0, len(train_data) - 1)\n",
    "print(f\"Target: {target_names[train_targets[ix]]}\\n\")\n",
    "print (train_data[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the classifier, first we are going to tokenize the dataset using spacy.io\n",
    "\n",
    "Run (shown in the cell below):\n",
    "\n",
    "* ```pip install spacy```\n",
    "* ```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.1.18)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "\u001b[31msmart-open 1.5.6 requires boto3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "# Let's write the tokenization function \n",
    "\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is, looking ! at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively try running the following multi-threaded version of tokenization\n",
    "# Credit to Ilya Kulikov\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 10000\n",
      "Val dataset size is 1314\n",
      "Test dataset size is 7532\n",
      "Total number of tokens in train dataset is 3433739\n"
     ]
    }
   ],
   "source": [
    "# First, download datasets from here\n",
    "# Use your NYU account\n",
    "#https://drive.google.com/open?id=1eR2LFI5MGliHlaL1S2nsX4ouIO1k_ip2\n",
    "#https://drive.google.com/open?id=133QCWbiz_Xc7Qm4r6t-fJP1K669xjNlM\n",
    "#https://drive.google.com/open?id=1SuUIUpJ1iznU707ktkpnEGSwt_XIqOYp\n",
    "#https://drive.google.com/open?id=1UQsrZ2LVfcxdxxa47344fMs_qvya72KR\n",
    "import pickle as pkl\n",
    "# Then, load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2135 ; token recommend\n",
      "Token recommend; token id 2135\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 10000\n",
      "Val dataset size is 1314\n",
      "Test dataset size is 7532\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/313], Validation Acc: 44.67275494672755\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 69.17808219178082\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 79.60426179604262\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 82.64840182648402\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 84.17047184170472\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 86.5296803652968\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 86.91019786910198\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 88.43226788432268\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 87.8234398782344\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 88.0517503805175\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 88.81278538812785\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 88.35616438356165\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 88.73668188736681\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 88.35616438356165\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 88.35616438356165\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 88.50837138508372\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 88.66057838660578\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 88.50837138508372\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 89.04109589041096\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 88.43226788432268\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 88.43226788432268\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 88.58447488584476\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 88.12785388127854\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 87.67123287671232\n",
      "Test Acc 78.86351566648965\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1\n",
    "### Try training the model with larger embedding size and for larger number of epochs\n",
    "### Also plot the training curves of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Try downloading IMDB Large Movie Review Dataset that is used for Assignment 1 http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "### and tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "### If you have time, after tokenizing the dataset try training Bag-of-Words model on it and report your initial results\n",
    "### on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
