{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = list(np.zeros((25000,1))) + list(np.ones((25000,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "def shuffle_data(data, targets):    \n",
    "    data_zip = list(zip(data, targets))\n",
    "    random.shuffle(data_zip)\n",
    "    data_shuffle, targets_shuffle = list(zip(*train_zip))\n",
    "    return data_shuffle, targets_shuffle\n",
    "\n",
    "# load data\n",
    "data_path = 'data'\n",
    "\n",
    "def load_data(kind):\n",
    "    data = []\n",
    "    \n",
    "    for label in ('neg', 'pos'):\n",
    "        files = glob.glob(data_path + f\"/{kind}/{label}/*\")\n",
    "        assert(len(files) == 12500)\n",
    "        for text_file in files:\n",
    "            with open(text_file, 'r') as fin:\n",
    "                data.append(fin.readline().strip())\n",
    "                \n",
    "    assert(len(data) == 25000)\n",
    "    n_pos = 12500\n",
    "    targets = list(np.zeros((n_pos))) + list(np.ones((n_pos)))\n",
    "    \n",
    "    return shuffle_data(data, targets)\n",
    "\n",
    "train_data_pre, train_targets_pre = load_data('train')\n",
    "test_data, test_targets = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 25000\n",
      "Test dataset size is 25000\n",
      "After val split\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# Split train data into actual train and validation sets\n",
    "print (\"Train dataset size is {}\".format(len(train_data_pre)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))\n",
    "\n",
    "train_split = 20000\n",
    "\n",
    "val_data = train_data_pre[train_split:]\n",
    "val_targets = train_targets_pre[train_split:]\n",
    "\n",
    "train_data = train_data_pre[:train_split]\n",
    "train_targets = train_targets_pre[:train_split]\n",
    "\n",
    "print(\"After val split\")\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1.0\n",
      "\n",
      "I decided to watch this because of the recommendations from this site. I would have to say it was worth the effort. However, you should take heed that this film will go on for 210 minutes. If you don't have the staying power, get it on tape and watch it over a couple of nights.<br /><br />Now to the film, what I say will contain \"spoilers\" and if you don't mind, here goes: <br /><br />Alexandre is a promiscuous bum, a womanizer and a gigolo. He lives with an older woman called Marie. Marie owns a retail shop and she provides for Alex. Alex spends his days at cafés and restaurants. The story reveals that Alex had previously impregnated Gilberte whom he used to live with. Gilberte dumped him for a less attractive man that she did not love because Alex had abused and battered her. At this point, Alex was willing to get a job and and help raise their child before he found out Gilberte had aborted it and planned to marry someone else. <br /><br />By chance, Alexandre meets a nurse nymph called Veronika and they striked up a relationship. Veronika fell in love with Alex for the first time after all the sordid sex she had with men in the past. Marie and Veronika struggles for Alex's affection and had a ménage à trois to boot. Finally at the end, it's revealed Veronika is pregnant with Alex's child and Alex asked her to marry him. We assume (as aforesaid with Gilberte's situation) Alexandre will even get a job and be the provider for his new found love and family. There is hope! <br /><br />With the title of \"La Maman et la putain\", I deduce Jean Eustache was relating to Françoise Lebrun's character of Veronika. She was a whore and then she became the mother. Hence, the mother and whore is the same person? Anyway, what do I know! French films are mostly (not all) very chatty, aimlessly political, preaching, theatrical, insipid, lamenting and full of quotes. Lebrun and Léaud played their obdurate characters well and held the film together as some part of the script became a little lost and disjointed. <br /><br />Not a bad effort. 7/10.\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "ix = random.randint(0, len(train_data) - 1)\n",
    "print(f\"Target: {train_targets[ix]}\\n\")\n",
    "print (train_data[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the classifier, first we are going to tokenize the dataset using spacy.io\n",
    "\n",
    "Run (shown in the cell below):\n",
    "\n",
    "* ```pip install spacy```\n",
    "* ```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (2.0.12)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.15.2)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (6.10.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (0.2.8.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
      "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.3.2)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.1.18)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
      "\u001b[31msmart-open 1.5.6 requires boto3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /Users/diogomesquita/anaconda/envs/py3.6/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data \n",
    "\n",
    "train_data_ix_{vocab_size}_{n}gram.pickle val_data_ix_{vocab_size}_{n}gram.pickle test_data_ix_{vocab_size}_{n}gram.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is, looking ! at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code cell that tokenizes train/val/test datasets\n",
    "# However it takes about 15-20 minutes to run it\n",
    "# For convinience we have provided the preprocessed datasets\n",
    "# Please see the next code cell\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "#pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c2cb198d2e4c67be57fa569c4cc8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051ae907c6e3412482b3bc7437900caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf4c1f080954d10a8518c2d4e993808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train 1grams:  4795739\n",
      "Number of train 2grams:  4775739\n",
      "Number of train 3grams:  4755739\n",
      "Number of train 4grams:  4735739\n"
     ]
    }
   ],
   "source": [
    "# Alternatively try running the following multi-threaded version of tokenization\n",
    "# Credit to Ilya Kulikov\n",
    "import pickle as pkl\n",
    "import tqdm\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_1gram = []\n",
    "    dataset_2gram = []\n",
    "    dataset_3gram = []\n",
    "    dataset_4gram = []\n",
    "    \n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_1gram = []\n",
    "    all_2gram = []\n",
    "    all_3gram = []\n",
    "    all_4gram = []\n",
    "\n",
    "    for sample in tqdm.tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=4)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        \n",
    "        dataset_1gram.append(tokens)\n",
    "        all_1gram += tokens\n",
    "        \n",
    "        sent_2gram = [' '.join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "        dataset_2gram.append(sent_2gram)\n",
    "        all_2gram += sent_2gram\n",
    "        \n",
    "        sent_3gram = [' '.join(tokens[i:i+3]) for i in range(len(tokens)-2)]\n",
    "        dataset_3gram.append(sent_3gram)\n",
    "        all_3gram += sent_3gram\n",
    "        \n",
    "        sent_4gram = [' '.join(tokens[i:i+4]) for i in range(len(tokens)-3)]\n",
    "        dataset_4gram.append(sent_4gram)\n",
    "        all_4gram += sent_4gram\n",
    "        \n",
    "    return [(dataset_1gram, all_1gram),\n",
    "            (dataset_2gram, all_2gram),\n",
    "            (dataset_3gram, all_3gram),\n",
    "            (dataset_4gram, all_4gram)]\n",
    "            \n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "(val_data_1gram, _),\\\n",
    "(val_data_2gram, _),\\\n",
    "(val_data_3gram, _),\\\n",
    "(val_data_4gram, _) = tokenize_dataset(val_data)\n",
    "\n",
    "pkl.dump([val_data_1gram, val_targets], open(\"data/processed/val_data_1gram.p\", \"wb\"))\n",
    "pkl.dump([val_data_2gram, val_targets], open(\"data/processed/val_data_2gram.p\", \"wb\"))\n",
    "pkl.dump([val_data_3gram, val_targets], open(\"data/processed/val_data_3gram.p\", \"wb\"))\n",
    "pkl.dump([val_data_4gram, val_targets], open(\"data/processed/val_data_4gram.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "(test_data_1gram, _),\\\n",
    "(test_data_2gram, _),\\\n",
    "(test_data_3gram, _),\\\n",
    "(test_data_4gram, _) = tokenize_dataset(test_data)\n",
    "\n",
    "pkl.dump([test_data_1gram, test_targets], open(\"data/processed/test_data_1gram.p\", \"wb\"))\n",
    "pkl.dump([test_data_2gram, test_targets], open(\"data/processed/test_data_2gram.p\", \"wb\"))\n",
    "pkl.dump([test_data_3gram, test_targets], open(\"data/processed/test_data_3gram.p\", \"wb\"))\n",
    "pkl.dump([test_data_4gram, test_targets], open(\"data/processed/test_data_4gram.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "(train_data_1gram, all_train_1grams),\\\n",
    "(train_data_2gram, all_train_2grams),\\\n",
    "(train_data_3gram, all_train_3grams),\\\n",
    "(train_data_4gram, all_train_4grams) = tokenize_dataset(train_data)\n",
    "\n",
    "pkl.dump([train_data_1gram, train_targets], open(\"data/processed/train_data_1gram.p\", \"wb\"))\n",
    "pkl.dump([train_data_2gram, train_targets], open(\"data/processed/train_data_2gram.p\", \"wb\"))\n",
    "pkl.dump([train_data_3gram, train_targets], open(\"data/processed/train_data_3gram.p\", \"wb\"))\n",
    "pkl.dump([train_data_4gram, train_targets], open(\"data/processed/train_data_4gram.p\", \"wb\"))\n",
    "\n",
    "pkl.dump(all_train_1grams, open(\"data/processed/all_train_1grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_2grams, open(\"data/processed/all_train_2grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_3grams, open(\"data/processed/all_train_3grams.p\", \"wb\"))\n",
    "pkl.dump(all_train_4grams, open(\"data/processed/all_train_4grams.p\", \"wb\"))\n",
    "\n",
    "print(\"Number of train 1grams: \", len(all_train_1grams))\n",
    "print(\"Number of train 2grams: \", len(all_train_2grams))\n",
    "print(\"Number of train 3grams: \", len(all_train_3grams))\n",
    "print(\"Number of train 4grams: \", len(all_train_4grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert token to id in the dataset use the index_data.py file\n",
    "\n",
    "Usage: python index_data.py ngrams max_vocab_size\n",
    "\n",
    "Example: python index_data.py 2 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train the model use the train.py file.\n",
    "\n",
    "Usage: python train.py ngrams max_vocab_size embed_dim\n",
    "\n",
    "Example: python train.py 4 50000 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
